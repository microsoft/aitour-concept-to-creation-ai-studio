# Demo 4 - Evaluate your prototype 

The goal of this demo is to evaluate the performance of the prototype you built in the [previous demo](./demo3_add_your_own_data.md). Azure AI provides the ability to use AI-assisted evaluation tools to score the **generation quality** of your model. A common issue with generative AI models is that there's no ground truth to compare the generated text against, because they are non-deterministic. 

Azure AI helps you to overcome this limitation by instructing a second model of your choice - you are going to use *gpt-4* - to evaluate the input-output pairs generated by your flow, against pre-defined metrics.

In addition to the general pre-requisites defined in the [setup](./set_up.md) guidance, you need to install some additional dependencies to be able to execute your evaluation notebook.

1. Verify you have Python3 installed on your machine.
2. Install dependecies with `pip install -r requirements.txt`.
3. Install the [promptflow VS Code extension](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow).
4. Install the [python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) and [jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) extensions in VS Code, to run the evaluation notebook.

If you prefer, you can also rely on a pre-built environment which has all the dependecies already installed for you. Just click the button below to open this repo into a [GitHub Codespace](https://github.com/codespaces).

 [![Open in GitHub Codespaces](https://img.shields.io/static/v1?style=for-the-badge&label=GitHub+Codespaces&message=Open&color=brightgreen&logo=github)](https://github.com/codespaces/new?hide_repo_select=true&machine=basicLinux32gb&repo=826287138&ref=main&devcontainer_path=.devcontainer%2Fdevcontainer.json&geo=UsEast)

Also, in order to be able to visualize the evaluation results in Azure AI Studio make sure that:
 - you log in with your Azure AI account used to provision the Azure resources, by using the command `az login --use-device-code`
 - you assign to yourself the *Storage Blob Data Contributor* role to have access permissions to the Azure AI Project storage account `az role assignment create --role "Storage Blob Data Contributor" --scope /subscriptions/<mySubscriptionID>/resourceGroups/<myResourceGroupName> --assignee-principal-type User --assignee-object-id "<user-id>" ` 

 >[!NOTE]
 >If you used the [set_up.sh](./set_up.sh) script to provision your Azure AI Studio resources, the role assignment has been already done for you. 

## Evaluate and review the performance of your application

The first thing you need to evaluate your app is a test dataset. For the sake of this demo you are going to use the [sample dataset](./data/test_dataset.jsonl) provided in the data folder of this repository. 

Open the [generation_quality_eval.ipynb](./web_designer_app/generation_quality_eval.ipynb) notebook and run the notebook cells to evaluate the performance of your application flow. This notebook performs the following actions:
1. Configure the Azure OpenAI model instance and test dataset to be used in the evaluation.
1. Import class evaluators from the promptflow library to evaluate against the most common generation quality metrics - *coherence, relevance, groundedness and fluency*. You can find a definition for each of these metrics [here](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in?WT.mc_id=academic-145965-cacaste&tabs=warning#generation-quality-metrics).
1. Execute a batch run of the app flow against the test dataset, to generate the input-output pairs, where the ouput include the model response and the context retrieved from the products catalog.
1. Evaluate the generated pairs against the pre-defined metrics and generate a report with the results.

The evaluation results includes an average score per each metric and per each item of the test dataset, in a range from 0 to 5. You can use this detailed information to understand where your flow is performing well and where it needs improvement.

## Next step: app deployment

Evaluation is an iterative process, in which you use the results of an evaluation run to improve your application flow and then re-evaluate it. For the sake of this demo you used a very simple dataset, but in a real-world scenario you should use a dataset representative of the real data your application will be handling.
Once you are satisfied with the performance of your application flow, you can move to the next step, which is [deploying it to the Azure Cloud as an online endpoint](https://learn.microsoft.com/azure/machine-learning/prompt-flow/how-to-deploy-for-real-time-inference?WT.mc_id=academic-145965-cacaste). This will allow you to interact with your application flow in a production environment, and to monitor its performance and usage.
